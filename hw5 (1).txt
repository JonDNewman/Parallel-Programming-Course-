  /*Assume B_arr is in Column Major Order
    A_arr and C_arr are in Row-Major Order


    Square Matrices Only!!
  */
  #define N 80
  #include <string.h>
  #include <mpi.h>
  #include <stdio.h>
  #include <stdlib.h>
  int id;             // Rank of current processor
  int nprocs;         // Number of processors

void populate_matrix(int a[N][N])
{
    
    int i,j;
    for(i=0;i<N;i++)
    {
          for(j=0;j<N;j++)
          {
            a[i][j]=1;
          }
          
    }
}
void nodecompute(int offset, int size, int A[][N], int B[][N], int C[][N])
{
  //printf("id %d here\n", id);
  //fflush(stdout);
  int i,j,k;

  for(i=0; i<size; i++)
  {
    //printf("i=%d\n", i);
    for(j=0; j<size; j++)
    {
      //printf("j=%d\n", j);
     
      for(k=0; k<N; k++)
      {
        //printf("k=%d\n", k);
        C[i][j+offset]+=A[j][k]*B[j][k];
        //printf("C[%d][%d] , id =%d, j=%d, k=%d\n", i, j+offset, id, j, k);
        //fflush(stdout);
      }
    }
  } 
    
      
 }  
void printMatrix(int p[][N], int rows, int cols)
{
  int i,j;
  for(i=0; i<rows; i++)
  {
    for(j=0; j<cols; j++)
    {
   
      printf("%d ", p[i][j]);
      
    }
    printf("\n");
    fflush(stdout);
  }

}


int main(int argc, char *argv[])
  {

      int A_arr[N][N];
      int B_arr[N][N];//B_arr is transposed initially (just to save time)
      int C_arr[N][N];
      /*starting and resultant arrays in P0*/
      
      double elapsed_time; 
      /*for timing purposes*/

       MPI_Status status;
       MPI_Init(&argc, &argv);
       MPI_Comm_rank(MPI_COMM_WORLD, &id);
       MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
       /*MPI info*/
       int p, chunk,  rmainder;
       chunk=N/nprocs;
       rmainder=N%nprocs;
       int A[chunk][N], B[chunk][N], C[chunk][N];
       //printMatrix(C);
       memset(C[0], 0, chunk*N*(sizeof(int)));
        if(id==0)
			   if(nprocs == 1)
			   {
                
              printf("Run with more than 1 processor \n");  
				      fflush(stdout);
              MPI_Finalize();

         }       			   
         if(id ==0)
         {

            populate_matrix(A_arr); 
            populate_matrix(B_arr);

            memcpy(A[0], A_arr[0], sizeof(int)*N*chunk);
            memcpy(B[0], B_arr[0], sizeof(int)*N*chunk);
            /*Start the timer*/
            elapsed_time = -MPI_Wtime();
            for(p = 1; p < nprocs; p++)
            {

              MPI_Send(A_arr[p*chunk],N*chunk,MPI_INT,p, p,MPI_COMM_WORLD);
              MPI_Send(B_arr[p*chunk],N*chunk,MPI_INT,p, p,MPI_COMM_WORLD);

            }

              // printf("p0 done sending init arrays\n");
              //     fflush(stdout);

         }
     if(id!=0)
      {
          
		     /*All other processes except P0 receive their part of the array*/
		     MPI_Recv(A[0],N*chunk, MPI_INT, 0, id, MPI_COMM_WORLD, &status);
         MPI_Recv(B[0],N*chunk, MPI_INT, 0, id, MPI_COMM_WORLD, &status);
         
        
        // printf("id = %d\n", id);
         //fflush(stdout);
           
      }

         int i,j;
				 if(id%2==0)
          {
             for(i=1; i< nprocs; i++)
             {

                 nodecompute((i-1)*chunk, chunk, A, B,C);
                 //printMatrix(C, chunk, N);
                 MPI_Send(B[0],N*chunk,MPI_INT,id!=nprocs-1? id+1:0, id!=nprocs-1? id+1:0, MPI_COMM_WORLD);
                 MPI_Recv(B[0],N*chunk, MPI_INT,  id!=0? id-1: nprocs-1, id, MPI_COMM_WORLD, &status);
                    
            
             }
              //again after all send/recvs
              
                 nodecompute((i-1)*chunk,chunk, A, B,C);
                 //printMatrix(C, chunk, N);
               //  printMatrix(C, chunk, N);
              //   printf("Chunk = %d\n", chunk);
         }
         else
         {
            for(i=1; i<nprocs; i++)
            {
                //printMatrix(C, chunk, N);
                nodecompute((i-1)*chunk,chunk, A, B,C);
          //      printMatrix(C, chunk, N);
                MPI_Recv(B[0],N*chunk, MPI_INT,  id-1, id, MPI_COMM_WORLD, &status);
                MPI_Send(B[0],N*chunk,MPI_INT, id!=nprocs-1? id+1:0, id!=nprocs-1? id+1:0, MPI_COMM_WORLD);
               
            }
           //  printf("i-1*chunk = %d", (i-1)*chunk);   
            nodecompute((i-1)*chunk,chunk, A, B,C);
          // printMatrix(C, chunk, N);
         

          }    			 
          if(id!=0)
            {
				      MPI_Send(C[0], N*chunk, MPI_INT, 0, id, MPI_COMM_WORLD);
            }
            else
             {
               for(p = 1; p < nprocs; p++)
                {
                  MPI_Recv(C_arr[p*chunk],N*chunk, MPI_INT, p, p, MPI_COMM_WORLD, &status);
                } 
               memcpy(C_arr[0], C[0], N*sizeof(int)*chunk);          
             }				 	   
			   if(id == 0)
        { 
          printf("N= %d  elapsed time = %f.\n", N, elapsed_time + MPI_Wtime());
//          printMatrix(C_arr, N, N);
        }
         
         MPI_Finalize();

  }
  


